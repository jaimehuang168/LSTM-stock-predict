# -*- coding: utf-8 -*-
"""NN project main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1057860DYSrM3Acik42_vcct9kCsNyLPM
"""

# clean old dirc
# %cd ..
!rm -r LSTM-stock-predict
!rm LSTM-stock-predict.zip
!rm -r __MACOSX
!ls

# after uploading
!unzip LSTM-stock-predict.zip
# %cd LSTM-stock-predict

import os
import json
import time
import math
import matplotlib.pyplot as plt
import pickle
from core.data_processor import DataLoader
from core.model import Model
from sklearn.metrics import r2_score


def plot_results(predicted_data, true_data):
    fig = plt.figure(facecolor='white')
    ax = fig.add_subplot(111)
    ax.plot(true_data, label='True Data')
    plt.plot(predicted_data, label='Prediction')
    plt.legend()
    plt.show()


def plot_results_multiple(predicted_data, true_data, prediction_len):
    print("plotting...")
    fig = plt.figure(facecolor='white')
    ax = fig.add_subplot(111)
    ax.plot(true_data, label='True Data')
	# Pad the list of predictions to shift it in the graph to it's correct start
    for i, data in enumerate(predicted_data):
        padding = [None for p in range(i * prediction_len)]
        plt.plot(padding + data, label='Prediction')
        plt.legend()
    plt.show()


# load the configs
configs = json.load(open('config.json', 'r'))
if not os.path.exists(configs['model']['save_dir']): os.makedirs(configs['model']['save_dir'])

data = DataLoader(
    os.path.join('data', configs['data']['filename']),
    configs['data']['train_test_split'],
    configs['data']['columns']
)

# point by point model
model_PBP = Model()
model_PBP.build_model(configs)

# multi sequence model
model_MS = Model()
model_MS.build_model(configs)

x, y = data.get_train_data(
    seq_len=configs['data']['sequence_length'],
    normalise=configs['data']['normalise']
)

# out-of memory generative training
steps_per_epoch = math.ceil((data.len_train - configs['data']['sequence_length']) / configs['training']['batch_size'])

# start training each model
model_PBP.train_generator(
    data_gen=data.generate_train_batch(
        seq_len=configs['data']['sequence_length'],
        batch_size=configs['training']['batch_size'],
        normalise=configs['data']['normalise']
    ),
    epochs=configs['training']['epochs'],
    batch_size=configs['training']['batch_size'],
    steps_per_epoch=steps_per_epoch,
    save_dir=configs['model']['save_dir']
)

model_MS.train_generator(
    data_gen=data.generate_train_batch(
        seq_len=configs['data']['sequence_length'],
        batch_size=configs['training']['batch_size'],
        normalise=configs['data']['normalise']
    ),
    epochs=configs['training']['epochs'],
    batch_size=configs['training']['batch_size'],
    steps_per_epoch=steps_per_epoch,
    save_dir=configs['model']['save_dir']
)

x_test, y_test = data.get_test_data(
    seq_len=configs['data']['sequence_length'],
    normalise=configs['data']['normalise']
)


# start predicting
# predictions = model.predict_sequence_full(x_test, configs['data']['sequence_length'])
predictions_PBP = model.predict_point_by_point(x_test)
predictions_MS = model.predict_sequences_multiple(x_test, configs['data']['sequence_length'], configs['data']['sequence_length'])


#plot_results(predictions_PBP, y_test)
#plot_results_multiple(predictions_MS, y_test, configs['data']['sequence_length'])

# Save model
pickle.dump(model_PBP, open("model_PBP.pkl", 'wb'))
pickle.dump(model_MS, open("model_MS.pkl", 'wb'))
#print("The R2 score on the Test set is:\t{:0.3f}".format(r2_score(y_test, predictions)))
#model.evaluate(x_test, y_test)

# plotting point-by-point predictions with/without denormalization

# denormalize point by point
x_test_true, y_test_true =data.get_test_data(
    seq_len=configs['data']['sequence_length'],
    normalise=False
)


predictions_PBP_true = []
for i, p in enumerate(predictions_PBP):
  predictions_PBP_true.append((1+p) * data.data_test[i][0])
  
# calculate mse
sigma = 0
for i in range(650):
  sigma += (y_test_true[i][0] - predictions_PBP_true[i]) ** 2
test_loss = model_PBP.model.evaluate(x_test, y_test)
print("test loss :", test_loss)
print("denormalized test mse loss:", sigma/len(predictions_PBP_true))

print("before de-normalization :")
plot_results(predictions_PBP, y_test)
fig = plt.figure(facecolor='white')
ax = fig.add_subplot(111)
print("after de-normalization :")
ax.plot(y_test_true, label='True Data')
plt.plot(predictions_true, label='Prediction')
plt.legend()
plt.show()

# plotting multi-sequence predictions with/without denormalization

import numpy as np
def denormalise_windows(original_data, window_data, single_window=False):
  '''De-Normalise windows'''
  denormalised_data = []  
  window_data = np.array(window_data).astype(float)
  
  window_data = [window_data] if single_window else window_data
  for i, window in enumerate(window_data):
    denormalised_window = []
    p0 = original_data[i*50][0]
    for col_i in window:
      denormalised_col = (col_i+1) * p0
      denormalised_window.append(denormalised_col)
    denormalised_data.append(denormalised_window)
  return denormalised_data

def plot_results_multiple2(predicted_data, true_data, prediction_len, height, width):
    fig = plt.figure(figsize=(8, width), facecolor='white')
    ax = fig.add_subplot(111)
    ax.plot(true_data, label='True Data')
	# Pad the list of predictions to shift it in the graph to it's correct start
    
    for i, d in enumerate(predicted_data):
        padding = [None for p in range(i * prediction_len)]
        plt.plot(padding + d, label='Prediction')
        plt.legend()
    plt.show()

  
x_test_true, y_test_true =data.get_test_data(
    seq_len=configs['data']['sequence_length'],
    normalise=False
)

predictions_MS_true = denormalise_windows(data.data_test, predictions_MS, False)
#print(len(data.data_test))

# calculate mse
sigma = 0
for i in range(650):
  sigma += (y_test_true[i][0] - predictions_MS_true[i//50][i%50]) ** 2
test_loss = model_MS.model.evaluate(x_test, y_test)
print("test loss :", test_loss)
print("denormalized test mse loss:", sigma/650)


print("before de-normalization :")
plot_results_multiple2(predictions_MS, y_test, configs['data']['sequence_length'], 5, 9)
print("after de-normalization :")
plot_results_multiple2(predictions_MS_true, y_test_true, configs['data']['sequence_length'], 8, 7)